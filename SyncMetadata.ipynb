{"cells":[{"cell_type":"markdown","source":["Notebook: Sync Dataverse to OneLake\n","\n","Author: Jen Beiser\n","\n","Runtime: PySpark (Fabric Notebook)\n","\n","Frequency: Every 12 hours (Fabric Job Scheduler)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c78beb9-c250-4c33-aeef-e191c2d74552"},{"cell_type":"code","source":["import os\n","import json\n","import time\n","import uuid\n","import random\n","import requests\n","from typing import Dict, Any, List, Tuple, Optional, Set\n","\n","from pyspark.sql import SparkSession, Row, functions \n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col\n","from pyspark.sql.window import Window\n","\n","spark = SparkSession.builder.getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":36,"statement_ids":[36],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:10.9028565Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:10.904003Z","execution_finish_time":"2025-12-18T17:30:11.1920496Z","parent_msg_id":"4b0eac13-adb5-454f-af5e-b7ca34ae1359"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 36, Finished, Available, Finished)"},"metadata":{}}],"execution_count":34,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1951fd20-bd69-48f7-83d9-b8e568903d2f"},{"cell_type":"markdown","source":["## Config"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"226d2211-589d-4076-bd13-59011ff03410"},{"cell_type":"code","source":["API_VERSION = \"v9.2\"\n","SCHEMA = \"crm_meta\"\n","\n","ENVIRONMENTS = {\n","    \"UAT\": \"https://org5c6f9bc3.api.crm.dynamics.com\",\n","    # \"PROD\": \"https://<prod>.api.crm.dynamics.com\",\n","}\n","\n","TARGET_ENV = \"UAT\"\n","ORG_BASE = ENVIRONMENTS[TARGET_ENV]\n","\n","LOCALES = [1033, 1036, 3082]  # English, French, Spanish\n","\n","\n","# --- Entra ID (Service Principal) ---\n","TENANT_ID = os.environ.get(\"DV_TENANT_ID\", \"<TENANT_ID>\")\n","CLIENT_ID = os.environ.get(\"DV_CLIENT_ID\", \"<CLIENT_ID>\")\n","CLIENT_SECRET = os.environ.get(\"DV_CLIENT_SECRET\", None)\n","\n","MODE = os.environ.get(\"DV_MODE\", \"incremental\")  # \"full\" or \"incremental\"\n","\n","\n","# Control + logs\n","CONTROL_TABLE = f\"{SCHEMA}.retrievemetadatachanges_state\"\n","RUN_LOG_TABLE = f\"{SCHEMA}.run_logs\"\n","\n","# Target tables (Delta tables)\n","TBL_GLOBAL = f\"{SCHEMA}.globaloptionsetmetadata\"\n","TBL_OPTIONS = f\"{SCHEMA}.optionsetmetadata\"\n","TBL_STATE = f\"{SCHEMA}.statemetadata\"\n","TBL_STATUS = f\"{SCHEMA}.statusmetadata\"\n","TBL_TARGET = f\"{SCHEMA}.targetmetadata\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:10.9611268Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:11.1939049Z","execution_finish_time":"2025-12-18T17:30:11.5047113Z","parent_msg_id":"446ecd0d-90b7-43f6-8cc8-127ac66126f0"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 37, Finished, Available, Finished)"},"metadata":{}}],"execution_count":35,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bdb355b3-05d2-4a48-86b8-b7a853f3bfcc"},{"cell_type":"markdown","source":["## Schemas"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3d367c6-a121-4e4b-b9a7-412a5b86c781"},{"cell_type":"code","source":["control_schema = StructType([\n","    StructField(\"environment_name\", StringType(), True),\n","    StructField(\"scope_name\",       StringType(), True),\n","    StructField(\"server_version_stamp\", StringType(), True),\n","    StructField(\"last_run_ts\",      TimestampType(), True),\n","])\n","\n","global_schema = StructType([\n","    StructField(\"optionset_scope\",        StringType(),  True),\n","    StructField(\"optionset_name\",         StringType(),  True),\n","    StructField(\"entity_logical_name\",    StringType(),  True),\n","    StructField(\"attribute_logical_name\", StringType(),  True),\n","    StructField(\"attribute_metadata_id\",  StringType(),  True),\n","    StructField(\"option_value\",           IntegerType(), True),\n","    StructField(\"language_code\",          IntegerType(), True),\n","    StructField(\"label\",                  StringType(),  True),\n","])\n","\n","attr_schema = global_schema\n","\n","state_schema = StructType([\n","    StructField(\"entity_logical_name\",    StringType(),  True),\n","    StructField(\"attribute_logical_name\", StringType(),  True),\n","    StructField(\"attribute_metadata_id\",  StringType(),  True),\n","    StructField(\"option_value\",           IntegerType(), True),\n","    StructField(\"language_code\",          IntegerType(), True),\n","    StructField(\"label\",                  StringType(),  True),\n","])\n","\n","status_schema = state_schema\n","\n","target_schema = StructType([\n","    StructField(\"entity_logical_name\", StringType(), True),\n","    StructField(\"language_code\",       IntegerType(), True),\n","    StructField(\"label\",               StringType(), True),\n","])\n","\n","run_log_schema = StructType([\n","    StructField(\"run_id\",               StringType(),   True),\n","    StructField(\"environment_name\",     StringType(),   True),\n","    StructField(\"mode\",                 StringType(),   True),\n","    StructField(\"start_ts\",             TimestampType(),True),\n","    StructField(\"end_ts\",               TimestampType(),True),\n","    StructField(\"duration_seconds\",     LongType(),     True),\n","    StructField(\"global_row_count\",     LongType(),     True),\n","    StructField(\"options_row_count\",    LongType(),     True),\n","    StructField(\"state_row_count\",      LongType(),     True),\n","    StructField(\"status_row_count\",     LongType(),     True),\n","    StructField(\"target_row_count\",     LongType(),     True),\n","    StructField(\"deleted_attributes\",   IntegerType(),  True),\n","    StructField(\"server_version_stamp\", StringType(),   True),\n","    StructField(\"changed_entities\",     IntegerType(),  True),\n","])\n","\n","# Ensure schema + tables exist\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\")\n","spark.createDataFrame([], schema=control_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(CONTROL_TABLE)\n","spark.createDataFrame([], schema=run_log_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(RUN_LOG_TABLE)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.0264281Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:11.5066668Z","execution_finish_time":"2025-12-18T17:30:13.0324632Z","parent_msg_id":"434eef94-bba8-41f1-aaee-eb0c500458cb"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 38, Finished, Available, Finished)"},"metadata":{}}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ca5a30c-42e0-48f8-9ad1-af92bd28478f"},{"cell_type":"markdown","source":["## Helpers"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9efd8f1-3f31-41a6-a15c-148fd83187b5"},{"cell_type":"code","source":["def _sleep_with_jitter(base_seconds: float):\n","    time.sleep(base_seconds + random.random() * 0.25)\n","\n","def http_request_with_retry(method: str, url: str, *, headers: Dict[str, str], params=None, data=None, json_body=None,\n","                            timeout=60, max_attempts=6) -> requests.Response:\n","    attempt = 0\n","    while True:\n","        attempt += 1\n","        try:\n","            r = requests.request(\n","                method,\n","                url,\n","                headers=headers,\n","                params=params,\n","                data=data,\n","                json=json_body,\n","                timeout=timeout,\n","            )\n","\n","            # Throttling\n","            if r.status_code == 429:\n","                retry_after = r.headers.get(\"Retry-After\")\n","                wait = float(retry_after) if retry_after else min(2 ** attempt, 30)\n","                if attempt >= max_attempts:\n","                    r.raise_for_status()\n","                _sleep_with_jitter(wait)\n","                continue\n","\n","            # Transient server errors\n","            if r.status_code in (500, 502, 503, 504):\n","                if attempt >= max_attempts:\n","                    r.raise_for_status()\n","                _sleep_with_jitter(min(2 ** attempt, 30))\n","                continue\n","\n","            r.raise_for_status()\n","            return r\n","\n","        except requests.RequestException:\n","            if attempt >= max_attempts:\n","                raise\n","            _sleep_with_jitter(min(2 ** attempt, 30))\n","\n","def dv_headers(token: str) -> Dict[str, str]:\n","    return {\n","        \"Authorization\": f\"Bearer {token}\",\n","        \"Accept\": \"application/json\",\n","        \"OData-MaxVersion\": \"4.0\",\n","        \"OData-Version\": \"4.0\",\n","    }\n","\n","def dv_get_json(path_or_url: str, token: str, params: Dict[str, Any] = None) -> Dict[str, Any]:\n","    # Accept either a relative path or a full nextLink URL\n","    if path_or_url.startswith(\"http\"):\n","        url = path_or_url\n","    else:\n","        url = f\"{ORG_BASE}/api/data/{API_VERSION}/{path_or_url.lstrip('/')}\"\n","    r = http_request_with_retry(\"GET\", url, headers=dv_headers(token), params=params or {})\n","    return r.json()\n","\n","def dv_get_all(path: str, token: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n","    \"\"\"Follows @odata.nextLink and returns full aggregated 'value'.\"\"\"\n","    out: List[Dict[str, Any]] = []\n","    j = dv_get_json(path, token, params=params)\n","    out.extend(j.get(\"value\", []) or [])\n","    next_link = j.get(\"@odata.nextLink\")\n","    while next_link:\n","        j = dv_get_json(next_link, token)\n","        out.extend(j.get(\"value\", []) or [])\n","        next_link = j.get(\"@odata.nextLink\")\n","    return out"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":39,"statement_ids":[39],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.1559476Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:13.0345975Z","execution_finish_time":"2025-12-18T17:30:13.3939075Z","parent_msg_id":"01513185-786e-425e-8d8e-4c67db73860d"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 39, Finished, Available, Finished)"},"metadata":{}}],"execution_count":37,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"87b59522-b2a6-4fce-9b84-02d2c303d94e"},{"cell_type":"markdown","source":["## Auth"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ce9d69c-683f-4a9e-906d-bcd9bcb17388"},{"cell_type":"code","source":["def get_token() -> str:\n","    if not CLIENT_SECRET:\n","        raise RuntimeError(\"CLIENT_SECRET is missing. Set DV_CLIENT_SECRET env var or use a secret store.\")\n","    token_url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n","    data = {\n","        \"grant_type\": \"client_credentials\",\n","        \"client_id\": CLIENT_ID,\n","        \"client_secret\": CLIENT_SECRET,\n","        \"scope\": f\"{ORG_BASE}/.default\"\n","    }\n","    r = http_request_with_retry(\"POST\", token_url, headers={\"Accept\": \"application/json\"}, data=data)\n","    return r.json()[\"access_token\"]\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":40,"statement_ids":[40],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.2542392Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:13.3959669Z","execution_finish_time":"2025-12-18T17:30:13.6811204Z","parent_msg_id":"432870f9-2e23-4987-99ce-c4186bed1b9e"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 40, Finished, Available, Finished)"},"metadata":{}}],"execution_count":38,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c28e993-a2c5-482d-b391-f509ded16fd0"},{"cell_type":"markdown","source":["## Control table (ServerVersionStamp)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f2f1c3a-65ea-4172-b987-a8a6a16ae828"},{"cell_type":"code","source":["def get_server_stamp(env_name: str, scope: str) -> Optional[str]:\n","    df = spark.table(CONTROL_TABLE).where(\n","        (col(\"environment_name\") == env_name) & (col(\"scope_name\") == scope)\n","    )\n","    rows = df.limit(1).collect()\n","    return rows[0][\"server_version_stamp\"] if rows else None\n","\n","def upsert_server_stamp(env_name: str, scope: str, stamp: str):\n","    stamp_escaped = (stamp or \"\").replace(\"'\", \"''\")\n","    spark.sql(f\"\"\"\n","      MERGE INTO {CONTROL_TABLE} AS t\n","      USING (SELECT '{env_name}' AS environment_name,\n","                    '{scope}'     AS scope_name,\n","                    '{stamp_escaped}' AS server_version_stamp,\n","                    current_timestamp() AS last_run_ts) AS s\n","      ON t.environment_name = s.environment_name AND t.scope_name = s.scope_name\n","      WHEN MATCHED THEN UPDATE SET\n","          t.server_version_stamp = s.server_version_stamp,\n","          t.last_run_ts = s.last_run_ts\n","      WHEN NOT MATCHED THEN INSERT *\n","    \"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":41,"statement_ids":[41],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.3426397Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:13.6834042Z","execution_finish_time":"2025-12-18T17:30:13.9537977Z","parent_msg_id":"c5d30a95-5c4d-4fbf-907b-0b48a2645ab4"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 41, Finished, Available, Finished)"},"metadata":{}}],"execution_count":39,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a71d0739-5d2e-40e5-9cc6-fd957112a1ce"},{"cell_type":"markdown","source":["## RetrieveMetadataChanges (incremental marker)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"29d12529-f85b-48db-9186-2f63effc4294"},{"cell_type":"code","source":["ENUM_FQN = \"Microsoft.Dynamics.CRM.DeletedMetadataFilters\"\n","\n","def build_simple_entity_query() -> dict:\n","    return {\n","        \"@odata.type\": \"Microsoft.Dynamics.CRM.EntityQueryExpression\",\n","        \"Properties\": {\"PropertyNames\": [\"LogicalName\", \"DisplayName\", \"Attributes\"]},\n","        \"Criteria\": {\"FilterOperator\": \"And\", \"Conditions\": []},\n","        \"AttributeQuery\": {\n","            \"@odata.type\": \"Microsoft.Dynamics.CRM.AttributeQueryExpression\",\n","            \"Properties\": {\"PropertyNames\": [\"LogicalName\", \"AttributeType\"]},\n","            \"Criteria\": {\"FilterOperator\": \"And\", \"Conditions\": []},\n","        },\n","    }\n","\n","def retrieve_metadata_changes(token: str, query: dict, client_version_stamp: Optional[str] = None,\n","                              deleted_filters: Optional[str] = None) -> Dict[str, Any]:\n","    parts = [\"RetrieveMetadataChanges(Query=@p1\"]\n","    params = {\"@p1\": json.dumps(query, separators=(\",\", \":\"))}\n","\n","    if client_version_stamp:\n","        parts.append(\"ClientVersionStamp=@p2\")\n","        params[\"@p2\"] = f\"'{client_version_stamp}'\"\n","\n","    if deleted_filters:\n","        parts.append(\"DeletedMetadataFilters=@p3\")\n","        params[\"@p3\"] = f\"{ENUM_FQN}'{deleted_filters}'\"\n","\n","    path = \",\".join(parts) + \")\"\n","    url = f\"{ORG_BASE}/api/data/{API_VERSION}/{path}\"\n","    r = http_request_with_retry(\"GET\", url, headers=dv_headers(token), params=params)\n","    return r.json()\n","\n","def extract_changed_entity_logical_names(changed_entities: List[Dict[str, Any]]) -> List[str]:\n","    seen = set()\n","    out = []\n","    for e in changed_entities or []:\n","        n = e.get(\"LogicalName\")\n","        if n and n not in seen:\n","            seen.add(n)\n","            out.append(n)\n","    return out\n","\n","def parse_deleted_attribute_ids(resp: Dict[str, Any]) -> Set[str]:\n","    deleted_attr_ids: Set[str] = set()\n","    deleted = resp.get(\"DeletedMetadata\") or {}\n","\n","    # Common: {\"Attribute\": [ {MetadataId: ...}, ... ]}\n","    for item in (deleted.get(\"Attribute\") or []):\n","        mid = item.get(\"MetadataId\") or item.get(\"AttributeId\") or item.get(\"ObjectId\")\n","        if mid:\n","            deleted_attr_ids.add(mid)\n","\n","    # Defensive: {\"DeletedMetadataCollection\": [ {DeletedMetadataFilters: \"Attribute\", MetadataId: ...}, ... ]}\n","    for item in (deleted.get(\"DeletedMetadataCollection\") or []):\n","        if item.get(\"DeletedMetadataFilters\") in (\"Attribute\", \"Attributes\"):\n","            mid = item.get(\"MetadataId\") or item.get(\"AttributeId\") or item.get(\"ObjectId\")\n","            if mid:\n","                deleted_attr_ids.add(mid)\n","\n","    return deleted_attr_ids\n","\n","def retrieve_metadata_incremental(token: str) -> Tuple[str, List[str], Set[str]]:\n","    prev = get_server_stamp(TARGET_ENV, \"attributes\")\n","    if not prev:\n","        raise RuntimeError(\"No previous ServerVersionStamp found. Run MODE=full once to bootstrap.\")\n","\n","    resp = retrieve_metadata_changes(\n","        token,\n","        build_simple_entity_query(),\n","        client_version_stamp=prev,\n","        deleted_filters=\"Attribute\",  # singular\n","    )\n","\n","    new_stamp = resp.get(\"ServerVersionStamp\")\n","    if not new_stamp:\n","        raise RuntimeError(f\"RetrieveMetadataChanges returned no ServerVersionStamp. Keys: {list(resp.keys())}\")\n","\n","    changed_entities = resp.get(\"EntityMetadata\") or []\n","    changed_entity_names = extract_changed_entity_logical_names(changed_entities)\n","    deleted_attr_ids = parse_deleted_attribute_ids(resp)\n","    return new_stamp, changed_entity_names, deleted_attr_ids\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":42,"statement_ids":[42],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.4966478Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:13.9557413Z","execution_finish_time":"2025-12-18T17:30:14.2414564Z","parent_msg_id":"4ef9eaaf-d975-456b-a577-5f82710c658e"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 42, Finished, Available, Finished)"},"metadata":{}}],"execution_count":40,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"22f08507-df0d-4884-9345-1308eb63f6a2"},{"cell_type":"markdown","source":["## Extractors"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00498bbb-6e1c-49b4-8da3-9e7429ef8759"},{"cell_type":"code","source":["def extract_entity_display_labels(token: str, entity_logical: str) -> List[Dict[str, Any]]:\n","    ent = dv_get_json(\n","        f\"EntityDefinitions(LogicalName='{entity_logical}')\",\n","        token,\n","        params={\n","            \"$select\": \"LogicalName,DisplayName\",\n","            \"LabelLanguages\": \",\".join(str(l) for l in LOCALES),\n","        },\n","    )\n","    rows = []\n","    for ll in ((ent.get(\"DisplayName\") or {}).get(\"LocalizedLabels\") or []):\n","        if ll.get(\"LanguageCode\") in LOCALES:\n","            rows.append({\n","                \"entity_logical_name\": entity_logical,\n","                \"language_code\": ll.get(\"LanguageCode\"),\n","                \"label\": ll.get(\"Label\"),\n","            })\n","    return rows\n","\n","def extract_global_options(token: str) -> List[Dict[str, Any]]:\n","    out: List[Dict[str, Any]] = []\n","    defs = dv_get_all(\"GlobalOptionSetDefinitions\", token)\n","\n","    for g in defs:\n","        name = g.get(\"Name\")\n","        metadata_id = g.get(\"MetadataId\")\n","        if not metadata_id:\n","            continue\n","\n","        full = dv_get_json(\n","            f\"GlobalOptionSetDefinitions({metadata_id})\",\n","            token,\n","            params={\"$format\": \"application/json;odata.metadata=none\"},\n","        )\n","        for opt in (full.get(\"Options\") or []):\n","            labels = ((opt.get(\"Label\") or {}).get(\"LocalizedLabels\") or [])\n","            for ll in labels:\n","                if ll.get(\"LanguageCode\") in LOCALES:\n","                    out.append({\n","                        \"optionset_scope\": \"Global\",\n","                        \"optionset_name\": name,\n","                        \"entity_logical_name\": None,\n","                        \"attribute_logical_name\": None,\n","                        \"attribute_metadata_id\": None,\n","                        \"option_value\": opt.get(\"Value\"),\n","                        \"language_code\": ll.get(\"LanguageCode\"),\n","                        \"label\": ll.get(\"Label\"),\n","                    })\n","    return out\n","\n","def extract_entity_attribute_options(token: str, entity_logical: str) -> Tuple[List[Dict[str, Any]],\n","                                                                             List[Dict[str, Any]],\n","                                                                             List[Dict[str, Any]]]:\n","    \"\"\"Returns: picklist_rows, state_rows, status_rows for a single entity.\"\"\"\n","    picklist_rows: List[Dict[str, Any]] = []\n","    state_rows: List[Dict[str, Any]] = []\n","    status_rows: List[Dict[str, Any]] = []\n","\n","    # Picklists (single & multi) with expand\n","    for cast in [\n","        \"Microsoft.Dynamics.CRM.PicklistAttributeMetadata\",\n","        \"Microsoft.Dynamics.CRM.MultiSelectPicklistAttributeMetadata\",\n","    ]:\n","        attrs = dv_get_all(\n","            f\"EntityDefinitions(LogicalName='{entity_logical}')/Attributes/{cast}\",\n","            token,\n","            params={\"$select\": \"LogicalName,MetadataId\", \"$expand\": \"OptionSet,GlobalOptionSet\"},\n","        )\n","        for a in attrs:\n","            attr_logical = a.get(\"LogicalName\")\n","            attr_id = a.get(\"MetadataId\")\n","\n","            # Local OptionSet\n","            for opt in (((a.get(\"OptionSet\") or {}).get(\"Options\") or [])):\n","                labels = ((opt.get(\"Label\") or {}).get(\"LocalizedLabels\") or [])\n","                for ll in labels:\n","                    if ll.get(\"LanguageCode\") in LOCALES:\n","                        picklist_rows.append({\n","                            \"optionset_scope\": \"Attribute\",\n","                            \"optionset_name\": None,\n","                            \"entity_logical_name\": entity_logical,\n","                            \"attribute_logical_name\": attr_logical,\n","                            \"attribute_metadata_id\": attr_id,\n","                            \"option_value\": opt.get(\"Value\"),\n","                            \"language_code\": ll.get(\"LanguageCode\"),\n","                            \"label\": ll.get(\"Label\"),\n","                        })\n","\n","            # GlobalOptionSet attached\n","            for opt in (((a.get(\"GlobalOptionSet\") or {}).get(\"Options\") or [])):\n","                labels = ((opt.get(\"Label\") or {}).get(\"LocalizedLabels\") or [])\n","                for ll in labels:\n","                    if ll.get(\"LanguageCode\") in LOCALES:\n","                        picklist_rows.append({\n","                            \"optionset_scope\": \"GlobalAttached\",\n","                            \"optionset_name\": None,\n","                            \"entity_logical_name\": entity_logical,\n","                            \"attribute_logical_name\": attr_logical,\n","                            \"attribute_metadata_id\": attr_id,\n","                            \"option_value\": opt.get(\"Value\"),\n","                            \"language_code\": ll.get(\"LanguageCode\"),\n","                            \"label\": ll.get(\"Label\"),\n","                        })\n","\n","            \n","\n","    # StateAttributeMetadata: must expand OptionSet Options\n","    states = dv_get_all(\n","        f\"EntityDefinitions(LogicalName='{entity_logical}')/Attributes/Microsoft.Dynamics.CRM.StateAttributeMetadata\",\n","        token,\n","        params={\"$select\": \"LogicalName,MetadataId\", \"$expand\": \"OptionSet($select=Options)\"},\n","    )\n","    for a in states:\n","        attr_logical = a.get(\"LogicalName\")\n","        attr_id = a.get(\"MetadataId\")\n","        for opt in (((a.get(\"OptionSet\") or {}).get(\"Options\") or [])):\n","            labels = ((opt.get(\"Label\") or {}).get(\"LocalizedLabels\") or [])\n","            for ll in labels:\n","                if ll.get(\"LanguageCode\") in LOCALES:\n","                    state_rows.append({\n","                        \"entity_logical_name\": entity_logical,\n","                        \"attribute_logical_name\": attr_logical,\n","                        \"attribute_metadata_id\": attr_id,\n","                        \"option_value\": opt.get(\"Value\"),\n","                        \"language_code\": ll.get(\"LanguageCode\"),\n","                        \"label\": ll.get(\"Label\"),\n","                    })\n","\n","    # StatusAttributeMetadata: must expand OptionSet Options\n","    statuses = dv_get_all(\n","        f\"EntityDefinitions(LogicalName='{entity_logical}')/Attributes/Microsoft.Dynamics.CRM.StatusAttributeMetadata\",\n","        token,\n","        params={\"$select\": \"LogicalName,MetadataId\", \"$expand\": \"OptionSet($select=Options)\"},\n","    )\n","    for a in statuses:\n","        attr_logical = a.get(\"LogicalName\")\n","        attr_id = a.get(\"MetadataId\")\n","        for opt in (((a.get(\"OptionSet\") or {}).get(\"Options\") or [])):\n","            labels = ((opt.get(\"Label\") or {}).get(\"LocalizedLabels\") or [])\n","            for ll in labels:\n","                if ll.get(\"LanguageCode\") in LOCALES:\n","                    status_rows.append({\n","                        \"entity_logical_name\": entity_logical,\n","                        \"attribute_logical_name\": attr_logical,\n","                        \"attribute_metadata_id\": attr_id,\n","                        \"option_value\": opt.get(\"Value\"),\n","                        \"language_code\": ll.get(\"LanguageCode\"),\n","                        \"label\": ll.get(\"Label\"),\n","                    })\n","\n","    return picklist_rows, state_rows, status_rows\n","\n","\n","def full_extract(token: str) -> Tuple[List[Dict[str, Any]],\n","                                     List[Dict[str, Any]],\n","                                     List[Dict[str, Any]],\n","                                     List[Dict[str, Any]],\n","                                     List[Dict[str, Any]]]:\n","    # Entities (paged)\n","    entities = dv_get_all(\n","        \"EntityDefinitions\",\n","        token,\n","        params={\"$select\": \"LogicalName,DisplayName\", \"LabelLanguages\": \",\".join(str(l) for l in LOCALES)},\n","    )\n","    entity_logicals = [e.get(\"LogicalName\") for e in entities if e.get(\"LogicalName\")]\n","\n","    # Global options\n","    global_rows = extract_global_options(token)\n","\n","    # Target display labels\n","    target_rows: List[Dict[str, Any]] = []\n","    for e in entities:\n","        logical = e.get(\"LogicalName\")\n","        if not logical:\n","            continue\n","        for ll in ((e.get(\"DisplayName\") or {}).get(\"LocalizedLabels\") or []):\n","            if ll.get(\"LanguageCode\") in LOCALES:\n","                target_rows.append({\n","                    \"entity_logical_name\": logical,\n","                    \"language_code\": ll.get(\"LanguageCode\"),\n","                    \"label\": ll.get(\"Label\"),\n","                })\n","\n","    # Attribute-level\n","    picklist_rows: List[Dict[str, Any]] = []\n","    state_rows: List[Dict[str, Any]] = []\n","    status_rows: List[Dict[str, Any]] = []\n","\n","    for logical in entity_logicals:\n","        p, s, st = extract_entity_attribute_options(token, logical)\n","        picklist_rows.extend(p)\n","        state_rows.extend(s)\n","        status_rows.extend(st)\n","\n","    return global_rows, picklist_rows, state_rows, status_rows, target_rows\n","\n","\n","def incremental_extract(token: str, changed_entity_logicals: List[str]) -> Tuple[List[Dict[str, Any]],\n","                                                                                List[Dict[str, Any]],\n","                                                                                List[Dict[str, Any]],\n","                                                                                List[Dict[str, Any]]]:\n","    picklist_rows: List[Dict[str, Any]] = []\n","    state_rows: List[Dict[str, Any]] = []\n","    status_rows: List[Dict[str, Any]] = []\n","    target_rows: List[Dict[str, Any]] = []\n","\n","    for logical in changed_entity_logicals:\n","        # entity display labels for targetmetadata\n","        target_rows.extend(extract_entity_display_labels(token, logical))\n","\n","        # attribute-level rows\n","        p, s, st = extract_entity_attribute_options(token, logical)\n","        picklist_rows.extend(p)\n","        state_rows.extend(s)\n","        status_rows.extend(st)\n","\n","    return picklist_rows, state_rows, status_rows, target_rows"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":43,"statement_ids":[43],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.6075187Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:14.2443045Z","execution_finish_time":"2025-12-18T17:30:14.5761271Z","parent_msg_id":"50ef7771-e2b2-4557-abc3-b0349bc4775b"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 43, Finished, Available, Finished)"},"metadata":{}}],"execution_count":41,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"643e026c-adbc-47d1-9519-3e35e167dad2"},{"cell_type":"markdown","source":["## Writers"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"accc5ec8-e4a2-4bfc-bae3-ff96ccd8ecaf"},{"cell_type":"code","source":["\n","from typing import Any, Dict, List, Set\n","from delta.tables import DeltaTable\n","from pyspark.sql import functions as F, Window, Row\n","from pyspark.sql.types import StructType\n","from pyspark.sql import DataFrame\n","\n","\n","def _merge_delete_keys(table_name: str, delete_keys_df: DataFrame, key_cols: List[str]) -> None:\n","    \"\"\"\n","    Perform a Delta MERGE ... WHEN MATCHED THEN DELETE for the given key columns.\n","    \"\"\"\n","    if delete_keys_df.rdd.isEmpty():\n","        return\n","    tgt_delta = DeltaTable.forName(spark, table_name)\n","    # Build ON clause dynamically from key columns\n","    on_parts = [f\"t.{c} = d.{c}\" for c in key_cols]\n","    on_expr = \" AND \".join(on_parts)\n","    tgt_delta.alias(\"t\").merge(\n","        delete_keys_df.alias(\"d\"),\n","        on_expr\n","    ).whenMatchedDelete().execute()\n","\n","def write_delta_init(rows: List[Dict[str, Any]], table_name: str, schema_struct: StructType):\n","    df = spark.createDataFrame(rows, schema=schema_struct)\n","    # Cast safety\n","    for c, t in [(\"option_value\", \"int\"), (\"language_code\", \"int\")]:\n","        if c in df.columns:\n","            df = df.withColumn(c, F.col(c).cast(t))\n","    df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n","\n","def write_delta_incremental(\n","    picklist_rows,\n","    state_rows,\n","    status_rows,\n","    target_rows,\n","    deleted_attr_ids: Set[str]\n","):\n","    # -------------------------\n","    # 1) PICKLISTS (OptionSet)\n","    # -------------------------\n","    if picklist_rows:\n","        pick_df = spark.createDataFrame(picklist_rows, schema=attr_schema)\n","\n","        key_cols = [\n","            \"entity_logical_name\",\n","            \"attribute_logical_name\",\n","            \"attribute_metadata_id\",\n","            \"option_value\",\n","            \"language_code\",\n","        ]\n","\n","        # Priority: Attribute (best) > GlobalAttached (fallback) > everything else\n","        pick_df = pick_df.withColumn(\n","            \"_scope_pri\",\n","            F.when(F.col(\"optionset_scope\") == \"Attribute\", F.lit(0))\n","             .when(F.col(\"optionset_scope\") == \"GlobalAttached\", F.lit(1))\n","             .otherwise(F.lit(2))\n","        )\n","\n","        w = Window.partitionBy(*key_cols).orderBy(F.col(\"_scope_pri\").asc())\n","\n","        pick_df = (\n","            pick_df\n","            .withColumn(\"_rn\", F.row_number().over(w))\n","            .where(F.col(\"_rn\") == 1)\n","            .drop(\"_rn\", \"_scope_pri\")\n","        )\n","\n","        pick_df.createOrReplaceTempView(\"picklist_rows\")\n","\n","        spark.sql(f\"\"\"\n","        MERGE INTO {TBL_OPTIONS} AS tgt\n","        USING picklist_rows AS src\n","        ON  tgt.entity_logical_name    = src.entity_logical_name\n","        AND tgt.attribute_logical_name = src.attribute_logical_name\n","        AND tgt.attribute_metadata_id  = src.attribute_metadata_id\n","        AND tgt.option_value           = src.option_value\n","        AND tgt.language_code          = src.language_code\n","        WHEN MATCHED THEN UPDATE SET\n","            tgt.label           = src.label,\n","            tgt.optionset_scope = src.optionset_scope\n","        WHEN NOT MATCHED THEN INSERT *\n","        \"\"\")\n","\n","    # Anti-join delete for picklists (no subqueries)\n","    current_opts_df = spark.createDataFrame(\n","        picklist_rows, schema=attr_schema\n","    ).select(\"attribute_metadata_id\", \"option_value\", \"language_code\").dropDuplicates()\n","\n","    if not current_opts_df.rdd.isEmpty() or deleted_attr_ids:\n","        # Limit the scope to attributes touched in this batch\n","        touched_attr_ids_df = current_opts_df.select(\"attribute_metadata_id\").distinct()\n","        target_keys_df = spark.table(TBL_OPTIONS).select(\n","            \"attribute_metadata_id\", \"option_value\", \"language_code\"\n","        )\n","        if not touched_attr_ids_df.rdd.isEmpty():\n","            target_keys_df = target_keys_df.join(\n","                F.broadcast(touched_attr_ids_df), on=\"attribute_metadata_id\", how=\"inner\"\n","            )\n","\n","        # Keys present in target but missing from current -> delete\n","        delete_keys_df = target_keys_df.join(\n","            F.broadcast(current_opts_df),\n","            on=[\"attribute_metadata_id\", \"option_value\", \"language_code\"],\n","            how=\"left_anti\",\n","        )\n","\n","        # Also delete all options under attributes Dataverse reported deleted\n","        if deleted_attr_ids:\n","            deleted_attr_ids_df = spark.createDataFrame(\n","                [(x,) for x in deleted_attr_ids], [\"attribute_metadata_id\"]\n","            )\n","            delete_attr_rows_df = spark.table(TBL_OPTIONS).select(\n","                \"attribute_metadata_id\", \"option_value\", \"language_code\"\n","            ).join(F.broadcast(deleted_attr_ids_df), on=\"attribute_metadata_id\", how=\"inner\")\n","\n","            delete_keys_df = delete_keys_df.union(delete_attr_rows_df).dropDuplicates()\n","\n","        _merge_delete_keys(TBL_OPTIONS, delete_keys_df, [\"attribute_metadata_id\", \"option_value\", \"language_code\"])\n","\n","    # --------------\n","    # 2) STATE labels\n","    # --------------\n","    if state_rows:\n","        spark.createDataFrame(state_rows, schema=state_schema).createOrReplaceTempView(\"state_rows\")\n","        spark.sql(f\"\"\"\n","          MERGE INTO {TBL_STATE} AS tgt\n","          USING state_rows AS src\n","          ON  tgt.entity_logical_name    = src.entity_logical_name\n","          AND tgt.attribute_logical_name = src.attribute_logical_name\n","          AND tgt.attribute_metadata_id  = src.attribute_metadata_id\n","          AND tgt.option_value           = src.option_value\n","          AND tgt.language_code          = src.language_code\n","          WHEN MATCHED THEN UPDATE SET tgt.label = src.label\n","          WHEN NOT MATCHED THEN INSERT *\n","        \"\"\")\n","\n","        # Delete stale state labels (anti-join)\n","        current_state_keys = spark.createDataFrame(state_rows, schema=state_schema) \\\n","            .select(\"attribute_metadata_id\", \"option_value\", \"language_code\").dropDuplicates()\n","\n","        touched_attr_ids_df = current_state_keys.select(\"attribute_metadata_id\").distinct()\n","        state_target_keys = spark.table(TBL_STATE).select(\n","            \"attribute_metadata_id\", \"option_value\", \"language_code\"\n","        )\n","        if not touched_attr_ids_df.rdd.isEmpty():\n","            state_target_keys = state_target_keys.join(\n","                F.broadcast(touched_attr_ids_df), on=\"attribute_metadata_id\", how=\"inner\"\n","            )\n","\n","        delete_state_keys = state_target_keys.join(\n","            F.broadcast(current_state_keys),\n","            on=[\"attribute_metadata_id\", \"option_value\", \"language_code\"],\n","            how=\"left_anti\",\n","        )\n","\n","        # Attributes deleted -> delete all state labels for them\n","        if deleted_attr_ids:\n","            deleted_attr_ids_df = spark.createDataFrame([(x,) for x in deleted_attr_ids], [\"attribute_metadata_id\"])\n","            del_all_state = spark.table(TBL_STATE).select(\n","                \"attribute_metadata_id\", \"option_value\", \"language_code\"\n","            ).join(F.broadcast(deleted_attr_ids_df), on=\"attribute_metadata_id\", how=\"inner\")\n","            delete_state_keys = delete_state_keys.union(del_all_state).dropDuplicates()\n","\n","        _merge_delete_keys(TBL_STATE, delete_state_keys, [\"attribute_metadata_id\", \"option_value\", \"language_code\"])\n","\n","    # ---------------\n","    # 3) STATUS labels\n","    # ---------------\n","    if status_rows:\n","        spark.createDataFrame(status_rows, schema=status_schema).createOrReplaceTempView(\"status_rows\")\n","        spark.sql(f\"\"\"\n","          MERGE INTO {TBL_STATUS} AS tgt\n","          USING status_rows AS src\n","          ON  tgt.entity_logical_name    = src.entity_logical_name\n","          AND tgt.attribute_logical_name = src.attribute_logical_name\n","          AND tgt.attribute_metadata_id  = src.attribute_metadata_id\n","          AND tgt.option_value           = src.option_value\n","          AND tgt.language_code          = src.language_code\n","          WHEN MATCHED THEN UPDATE SET tgt.label = src.label\n","          WHEN NOT MATCHED THEN INSERT *\n","        \"\"\")\n","\n","        # Delete stale status labels (anti-join)\n","        current_status_keys = spark.createDataFrame(status_rows, schema=status_schema) \\\n","            .select(\"attribute_metadata_id\", \"option_value\", \"language_code\").dropDuplicates()\n","\n","        touched_attr_ids_df = current_status_keys.select(\"attribute_metadata_id\").distinct()\n","        status_target_keys = spark.table(TBL_STATUS).select(\n","            \"attribute_metadata_id\", \"option_value\", \"language_code\"\n","        )\n","        if not touched_attr_ids_df.rdd.isEmpty():\n","            status_target_keys = status_target_keys.join(\n","                F.broadcast(touched_attr_ids_df), on=\"attribute_metadata_id\", how=\"inner\"\n","            )\n","\n","        delete_status_keys = status_target_keys.join(\n","            F.broadcast(current_status_keys),\n","            on=[\"attribute_metadata_id\", \"option_value\", \"language_code\"],\n","            how=\"left_anti\",\n","        )\n","\n","        if deleted_attr_ids:\n","            deleted_attr_ids_df = spark.createDataFrame([(x,) for x in deleted_attr_ids], [\"attribute_metadata_id\"])\n","            del_all_status = spark.table(TBL_STATUS).select(\n","                \"attribute_metadata_id\", \"option_value\", \"language_code\"\n","            ).join(F.broadcast(deleted_attr_ids_df), on=\"attribute_metadata_id\", how=\"inner\")\n","            delete_status_keys = delete_status_keys.union(del_all_status).dropDuplicates()\n","\n","        _merge_delete_keys(TBL_STATUS, delete_status_keys, [\"attribute_metadata_id\", \"option_value\", \"language_code\"])\n","\n","    # -------------------------\n","    # 4) TARGET entity labels\n","    # -------------------------\n","    if target_rows:\n","        spark.createDataFrame(target_rows, schema=target_schema).createOrReplaceTempView(\"target_rows\")\n","        spark.sql(f\"\"\"\n","          MERGE INTO {TBL_TARGET} AS tgt\n","          USING target_rows AS src\n","          ON  tgt.entity_logical_name = src.entity_logical_name\n","          AND tgt.language_code       = src.language_code\n","          WHEN MATCHED THEN UPDATE SET tgt.label = src.label\n","          WHEN NOT MATCHED THEN INSERT *\n","        \"\"\")\n","\n","        # Delete stale entity labels (anti-join on (entity_logical_name, language_code))\n","        current_target_keys = spark.createDataFrame(target_rows, schema=target_schema) \\\n","            .select(\"entity_logical_name\", \"language_code\").dropDuplicates()\n","\n","        target_keys = spark.table(TBL_TARGET).select(\"entity_logical_name\", \"language_code\")\n","        # Limit to entities touched in this batch\n","        target_keys = target_keys.join(\n","            F.broadcast(current_target_keys.select(\"entity_logical_name\").distinct()),\n","            on=\"entity_logical_name\", how=\"inner\"\n","        )\n","\n","        delete_target_keys = target_keys.join(\n","            F.broadcast(current_target_keys),\n","            on=[\"entity_logical_name\", \"language_code\"],\n","            how=\"left_anti\",\n","        )\n","\n","        _merge_delete_keys(TBL_TARGET, delete_target_keys, [\"entity_logical_name\", \"language_code\"])\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":44,"statement_ids":[44],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.7866349Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:14.5783418Z","execution_finish_time":"2025-12-18T17:30:14.8750017Z","parent_msg_id":"da0af741-1b49-4906-93ef-9da7d3627650"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 44, Finished, Available, Finished)"},"metadata":{}}],"execution_count":42,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c21a1c1-f259-4449-9170-9faea5340f81"},{"cell_type":"markdown","source":["## Metrics + Logging"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1fab63de-35a5-474c-88bc-69f180ac2ff3"},{"cell_type":"code","source":["def table_count(tbl: str) -> int:\n","    try:\n","        return int(spark.sql(f\"SELECT COUNT(*) c FROM {tbl}\").collect()[0][\"c\"])\n","    except Exception:\n","        return 0\n","\n","def log_run(run_id: str, start_ts, end_ts, mode: str, deleted_attrs: int, changed_entities: int):\n","    duration_seconds = int((end_ts.timestamp() - start_ts.timestamp()))\n","    log_row = Row(\n","        run_id=run_id,\n","        environment_name=TARGET_ENV,\n","        mode=mode,\n","        start_ts=start_ts,\n","        end_ts=end_ts,\n","        duration_seconds=duration_seconds,\n","        global_row_count=table_count(TBL_GLOBAL),\n","        options_row_count=table_count(TBL_OPTIONS),\n","        state_row_count=table_count(TBL_STATE),\n","        status_row_count=table_count(TBL_STATUS),\n","        target_row_count=table_count(TBL_TARGET),\n","        deleted_attributes=int(deleted_attrs),\n","        server_version_stamp=(get_server_stamp(TARGET_ENV, \"attributes\") or \"\"),\n","        changed_entities=int(changed_entities),\n","    )\n","    spark.createDataFrame([log_row], schema=run_log_schema).write.mode(\"append\").format(\"delta\").saveAsTable(RUN_LOG_TABLE)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":45,"statement_ids":[45],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.8784162Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:14.876893Z","execution_finish_time":"2025-12-18T17:30:15.1433727Z","parent_msg_id":"2dee3802-c203-4cda-8a4a-7b7de844a3bc"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 45, Finished, Available, Finished)"},"metadata":{}}],"execution_count":43,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e073954-c4c8-4cbf-8718-0946d1fc481d"},{"cell_type":"markdown","source":["## Runner"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6241c282-044b-47bf-b59e-27f781ed0723"},{"cell_type":"code","source":["def ensure_target_tables_exist():\n","    # Create empty Delta tables if needed (so merges don't fail on first run)\n","    spark.createDataFrame([], schema=global_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(TBL_GLOBAL)\n","    spark.createDataFrame([], schema=attr_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(TBL_OPTIONS)\n","    spark.createDataFrame([], schema=state_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(TBL_STATE)\n","    spark.createDataFrame([], schema=status_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(TBL_STATUS)\n","    spark.createDataFrame([], schema=target_schema).write.mode(\"ignore\").format(\"delta\").saveAsTable(TBL_TARGET)\n","\n","def run():\n","    ensure_target_tables_exist()\n","\n","    run_id = f\"{uuid.uuid4()}_{TARGET_ENV}_{MODE}\"\n","    start_ts = spark.sql(\"SELECT current_timestamp() ts\").collect()[0][\"ts\"]\n","    t0 = time.time()\n","\n","    token = get_token()\n","\n","    deleted_attr_ids: Set[str] = set()\n","    changed_entity_logicals: List[str] = []\n","\n","    if MODE.lower() == \"full\":\n","        # Full extract\n","        global_rows, picklist_rows, state_rows, status_rows, target_rows = full_extract(token)\n","\n","        # Write\n","        write_delta_init(global_rows,  TBL_GLOBAL,  global_schema)\n","        write_delta_init(picklist_rows, TBL_OPTIONS, attr_schema)\n","        write_delta_init(state_rows,   TBL_STATE,   state_schema)\n","        write_delta_init(status_rows,  TBL_STATUS,  status_schema)\n","        write_delta_init(target_rows,  TBL_TARGET,  target_schema)\n","\n","        # Bootstrap stamp after successful full\n","        resp0 = retrieve_metadata_changes(token, build_simple_entity_query())\n","        stamp0 = resp0.get(\"ServerVersionStamp\")\n","        if stamp0:\n","            upsert_server_stamp(TARGET_ENV, \"attributes\", stamp0)\n","\n","    else:\n","        # Incremental marker\n","        new_stamp, changed_entity_logicals, deleted_attr_ids = retrieve_metadata_incremental(token)\n","\n","        # If nothing changed and no deletes, still update stamp? Usually yes (keeps in sync with server)\n","        # But only after successful \"no-op\" decision:\n","        picklist_rows, state_rows, status_rows, target_rows = incremental_extract(token, changed_entity_logicals)\n","\n","        # Always refresh global option sets (cheap + correct for choice edits)\n","        global_rows = extract_global_options(token)\n","        write_delta_init(global_rows, TBL_GLOBAL, global_schema)\n","\n","        # Apply merges/deletes\n","        write_delta_incremental(picklist_rows, state_rows, status_rows, target_rows, deleted_attr_ids)\n","\n","        # Persist new stamp only after successful writes\n","        upsert_server_stamp(TARGET_ENV, \"attributes\", new_stamp)\n","\n","    end_ts = spark.sql(\"SELECT current_timestamp() ts\").collect()[0][\"ts\"]\n","    log_run(run_id, start_ts, end_ts, MODE.lower(), deleted_attrs=len(deleted_attr_ids), changed_entities=len(changed_entity_logicals))\n","\n","    # Console summary\n","    print(\" Dataverse Metadata Sync Complete\")\n","    print(f\"  run_id            : {run_id}\")\n","    print(f\"  env               : {TARGET_ENV}\")\n","    print(f\"  mode              : {MODE.lower()}\")\n","    print(f\"  duration_seconds  : {int(time.time() - t0)}\")\n","    print(f\"  changed_entities  : {len(changed_entity_logicals)}\")\n","    print(f\"  deleted_attributes: {len(deleted_attr_ids)}\")\n","    print(f\"  stamp             : {get_server_stamp(TARGET_ENV, 'attributes')}\")\n","\n","run()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":46,"statement_ids":[46],"state":"finished","livy_statement_state":"available","session_id":"ca89b146-d56f-4647-aad5-c130a141d080","normalized_state":"finished","queued_time":"2025-12-18T17:30:11.9628664Z","session_start_time":null,"execution_start_time":"2025-12-18T17:30:15.1452467Z","execution_finish_time":"2025-12-18T17:31:25.1367797Z","parent_msg_id":"e9c0bfbb-332e-40b7-a833-8fad8f64af7a"},"text/plain":"StatementMeta(, ca89b146-d56f-4647-aad5-c130a141d080, 46, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Dataverse Metadata Sync Complete\n  run_id            : 008a41c9-1043-4988-8639-af151e25f1b8_UAT_incremental\n  env               : UAT\n  mode              : incremental\n  duration_seconds  : 66\n  changed_entities  : 1\n  deleted_attributes: 0\n  stamp             : 5934330!12/18/2025 17:30:18\n"]}],"execution_count":44,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba45b894-bdb6-4b01-bd2a-c26f95fca7d1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"98e2ab65-3e28-4bb9-afaa-ac5f306dcf9d"}],"default_lakehouse":"98e2ab65-3e28-4bb9-afaa-ac5f306dcf9d","default_lakehouse_name":"Dataverse","default_lakehouse_workspace_id":"98497b13-2551-4cbd-9ad9-b92d4583edd4"}}},"nbformat":4,"nbformat_minor":5}